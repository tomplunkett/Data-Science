{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Cycle Generative Adversarial Network\n\nThis implementation creates a CycleGAN (Cycle-Consistent Generative Adversarial Network) that transforms regular photographs into Monet-style paintings. The model learns to translate between two image domains without requiring paired training data.\n\nArchitecture\nThe CycleGAN consists of four neural networks:\n\nGenerator A→B: Converts photos to Monet-style images\nGenerator B→A: Converts Monet-style images back to photos\nDiscriminator A: Distinguishes real photos from generated photos\nDiscriminator B: Distinguishes real Monet paintings from generated ones\n\nTraining Process\n\nData Loading: Load Monet paintings and photos from GCS buckets\nModel Creation: Initialize generators and discriminators\nTraining Loop:\n\nFor each batch, compute forward pass\nCalculate adversarial and cycle consistency losses\nUpdate generator and discriminator weights alternately\n\n\nInference: Generate Monet-style versions of all photos\nExport: Save results and create downloadable archive","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T03:29:42.459078Z","iopub.execute_input":"2025-06-17T03:29:42.459402Z","iopub.status.idle":"2025-06-17T04:06:14.801344Z","shell.execute_reply.started":"2025-06-17T03:29:42.459376Z","shell.execute_reply":"2025-06-17T04:06:14.800254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport numpy as np\nfrom PIL import Image\nimport shutil\nfrom kaggle_datasets import KaggleDatasets\n\n# ============================================================================\n# CONFIGURATION PARAMETERS\n# ============================================================================\n\nIMAGE_SIZE = 256    # All images will be resized to 256x256 pixels\nBATCH_SIZE = 1      # Process one image at a time (memory efficient for large images)\nEPOCHS = 15         # Number of complete passes through the dataset\nLAMBDA = 10         # Weight for cycle consistency loss (higher = more faithful reconstruction)\n\nclass Config:\n    pass  # Placeholder for configuration - no longer needed for output directory\n\n# ============================================================================\n# SETUP AND UTILITY FUNCTIONS\n# ============================================================================\n\ndef setup_gpu():\n    \"\"\"\n    Configure GPU usage for optimal performance.\n    Enables memory growth to prevent TensorFlow from allocating all GPU memory at once.\n    \"\"\"\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        print(f'Using GPU: {len(gpus)} device(s)')\n        # Enable memory growth for each GPU to avoid memory allocation errors\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    else:\n        print('Using CPU')\n\ndef load_and_preprocess_image(path):\n    \"\"\"\n    Load and preprocess a single image for GAN training.\n    \n    Args:\n        path: String path to the image file\n        \n    Returns:\n        Preprocessed image tensor normalized to [-1, 1] range\n    \"\"\"\n    # Read image file as raw bytes\n    image = tf.io.read_file(path)\n    \n    # Decode JPEG to tensor with 3 color channels (RGB)\n    image = tf.image.decode_jpeg(image, channels=3)\n    \n    # Convert to float32 for neural network processing\n    image = tf.cast(image, tf.float32)\n    \n    # Resize to consistent dimensions (256x256)\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    \n    # Normalize from [0, 255] to [-1, 1] range (standard for GANs)\n    # This helps with training stability\n    return (image / 127.5) - 1.0\n\ndef create_dataset_from_gcs(file_pattern, batch_size=BATCH_SIZE):\n    \"\"\"\n    Create a TensorFlow dataset from Google Cloud Storage file pattern.\n    \n    Args:\n        file_pattern: Glob pattern to match files (e.g., 'gs://bucket/folder/*.jpg')\n        batch_size: Number of images per batch\n        \n    Returns:\n        tf.data.Dataset: Preprocessed and batched dataset\n    \"\"\"\n    # Find all files matching the pattern\n    files = tf.io.gfile.glob(file_pattern)\n    if not files:\n        raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n    \n    print(f\"Found {len(files)} files matching pattern: {file_pattern}\")\n    \n    # Create dataset from file paths\n    dataset = tf.data.Dataset.from_tensor_slices(files)\n    \n    # Apply preprocessing to each image in parallel for speed\n    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Batch the data and prefetch next batches while processing current batch\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\n# ============================================================================\n# NEURAL NETWORK BUILDING BLOCKS\n# ============================================================================\n\ndef conv_block(filters, size, strides=2, norm=True, activation='leaky', transpose=False):\n    \"\"\"\n    Create a reusable convolutional block for generators and discriminators.\n    \n    Args:\n        filters: Number of output channels/filters\n        size: Kernel size (typically 4x4)\n        strides: Stride for convolution (2 = downsample, 1 = same size)\n        norm: Whether to apply batch normalization\n        activation: Type of activation ('leaky' or 'relu')\n        transpose: If True, use transposed convolution for upsampling\n        \n    Returns:\n        tf.keras.Sequential: Complete convolutional block\n    \"\"\"\n    block = tf.keras.Sequential()\n    \n    if transpose:  \n        # Transposed convolution for upsampling (decoder/generator)\n        block.add(tf.keras.layers.Conv2DTranspose(filters, size, strides, padding='same', use_bias=False))\n    else:  \n        # Regular convolution for downsampling (encoder/discriminator)\n        block.add(tf.keras.layers.Conv2D(filters, size, strides, padding='same', use_bias=False))\n    \n    if norm:\n        # Batch normalization helps with training stability\n        # Note: Usually skipped in the first discriminator layer\n        block.add(tf.keras.layers.BatchNormalization())\n    \n    # Choose activation function\n    if activation == 'leaky':\n        # LeakyReLU allows small negative values (good for discriminators)\n        block.add(tf.keras.layers.LeakyReLU(0.2))\n    elif activation == 'relu':\n        # Standard ReLU (good for generators)\n        block.add(tf.keras.layers.ReLU())\n    \n    return block\n\ndef build_generator():\n    \"\"\"\n    Build a U-Net style generator for image-to-image translation.\n    \n    U-Net architecture:\n    - Encoder: Downsamples input image to capture high-level features\n    - Decoder: Upsamples back to original size while preserving spatial details\n    - Skip connections: Connect encoder to decoder to preserve spatial information\n    \n    Returns:\n        tf.keras.Model: Generator model that transforms images between domains\n    \"\"\"\n    # Input: 256x256x3 image\n    inputs = tf.keras.layers.Input([IMAGE_SIZE, IMAGE_SIZE, 3])\n    \n    # ========== ENCODER (DOWNSAMPLING PATH) ==========\n    # Each layer reduces spatial dimensions by half, increases channels\n    d1 = conv_block(64, 4, norm=False)(inputs)     # 128x128x64 (no norm in first layer)\n    d2 = conv_block(128, 4)(d1)                    # 64x64x128\n    d3 = conv_block(256, 4)(d2)                    # 32x32x256\n    d4 = conv_block(512, 4)(d3)                    # 16x16x512\n    d5 = conv_block(512, 4)(d4)                    # 8x8x512 (bottleneck)\n    \n    # ========== DECODER (UPSAMPLING PATH) ==========\n    # Each layer doubles spatial dimensions, includes skip connections\n    \n    # First decoder layer: 8x8 -> 16x16\n    u1 = conv_block(512, 4, strides=2, activation='relu', transpose=True)(d5)  # 16x16x512\n    u1 = tf.keras.layers.Concatenate()([u1, d4])    # Skip connection: combine with d4\n    \n    # Second decoder layer: 16x16 -> 32x32\n    u2 = conv_block(256, 4, strides=2, activation='relu', transpose=True)(u1)  # 32x32x256\n    u2 = tf.keras.layers.Concatenate()([u2, d3])    # Skip connection: combine with d3\n    \n    # Third decoder layer: 32x32 -> 64x64\n    u3 = conv_block(128, 4, strides=2, activation='relu', transpose=True)(u2)  # 64x64x128\n    u3 = tf.keras.layers.Concatenate()([u3, d2])    # Skip connection: combine with d2\n    \n    # Fourth decoder layer: 64x64 -> 128x128\n    u4 = conv_block(64, 4, strides=2, activation='relu', transpose=True)(u3)   # 128x128x64\n    u4 = tf.keras.layers.Concatenate()([u4, d1])    # Skip connection: combine with d1\n    \n    # ========== OUTPUT LAYER ==========\n    # Final layer: 128x128 -> 256x256x3\n    # Tanh activation produces output in [-1, 1] range (matching input normalization)\n    output = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(u4)\n    \n    return tf.keras.Model(inputs, output)\n\ndef build_discriminator():\n    \"\"\"\n    Build a PatchGAN discriminator for image classification.\n    \n    Unlike traditional discriminators that output a single real/fake prediction,\n    PatchGAN outputs predictions for overlapping patches of the image.\n    This focuses on local texture patterns rather than global structure.\n    \n    Returns:\n        tf.keras.Model: Discriminator model that classifies image patches\n    \"\"\"\n    # Input: 256x256x3 image\n    inputs = tf.keras.layers.Input([IMAGE_SIZE, IMAGE_SIZE, 3])\n    \n    # ========== DOWNSAMPLING LAYERS ==========\n    # Each layer reduces spatial size and increases feature depth\n    x = conv_block(64, 4, norm=False)(inputs)      # 128x128x64 (no batch norm in first layer)\n    x = conv_block(128, 4)(x)                      # 64x64x128\n    x = conv_block(256, 4)(x)                      # 32x32x256\n    x = conv_block(512, 4, strides=1)(x)           # 32x32x512 (no downsampling in last conv)\n    \n    # ========== OUTPUT LAYER ==========\n    # Output raw logits for each patch (no activation)\n    # Each output value corresponds to real/fake prediction for a patch\n    output = tf.keras.layers.Conv2D(1, 4, padding='same')(x)    # 32x32x1\n    \n    return tf.keras.Model(inputs, output)\n\n# ============================================================================\n# CYCLEGAN MODEL IMPLEMENTATION\n# ============================================================================\n\nclass SimpleCycleGAN(tf.keras.Model):\n    \"\"\"\n    Complete CycleGAN implementation with training logic.\n    \n    CycleGAN learns to translate between two image domains (A and B) without paired data.\n    It uses two generators (A->B and B->A) and two discriminators (for A and B).\n    \n    Key innovation: Cycle consistency loss ensures that A->B->A = A (and vice versa).\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize all four networks of the CycleGAN.\"\"\"\n        super().__init__()\n        \n        # ========== GENERATORS ==========\n        self.gen_AB = build_generator()  # Transforms domain A to B (photo -> Monet)\n        self.gen_BA = build_generator()  # Transforms domain B to A (Monet -> photo)\n        \n        # ========== DISCRIMINATORS ==========\n        self.disc_A = build_discriminator()  # Distinguishes real vs fake domain A images\n        self.disc_B = build_discriminator()  # Distinguishes real vs fake domain B images\n        \n    def compile(self, gen_opt, disc_opt):\n        \"\"\"\n        Configure optimizers for training.\n        \n        Args:\n            gen_opt: Optimizer for generators\n            disc_opt: Unused (kept for compatibility)\n        \"\"\"\n        super().compile()\n        self.gen_opt = gen_opt\n        \n        # Create separate optimizers for each discriminator\n        # Learning rate 2e-4 and beta_1=0.5 are standard for GANs\n        self.disc_A_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n        self.disc_B_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n        \n    @tf.function  # Compile for faster execution\n    def train_step(self, batch):\n        \"\"\"\n        Execute one training step of the CycleGAN.\n        \n        Args:\n            batch: Tuple of (real_A, real_B) image batches\n            \n        Returns:\n            Dict of loss values for monitoring training progress\n        \"\"\"\n        real_A, real_B = batch\n        \n        # Use persistent tape to compute multiple gradients\n        with tf.GradientTape(persistent=True) as tape:\n            \n            # ========== FORWARD PASS ==========\n            # Generate fake images in opposite domains\n            fake_B = self.gen_AB(real_A, training=True)    # Photo -> Monet style\n            fake_A = self.gen_BA(real_B, training=True)    # Monet -> Photo style\n            \n            # ========== CYCLE CONSISTENCY ==========\n            # Reconstruct original images (should match input)\n            cycled_A = self.gen_BA(fake_B, training=True)  # Photo -> Monet -> Photo\n            cycled_B = self.gen_AB(fake_A, training=True)  # Monet -> Photo -> Monet\n            \n            # ========== DISCRIMINATOR PREDICTIONS ==========\n            # Get discriminator outputs for real and fake images\n            disc_real_A = self.disc_A(real_A, training=True)\n            disc_real_B = self.disc_B(real_B, training=True)\n            disc_fake_A = self.disc_A(fake_A, training=True)\n            disc_fake_B = self.disc_B(fake_B, training=True)\n            \n            # ========== GENERATOR LOSSES ==========\n            # Adversarial loss: generators try to fool discriminators\n            # Use binary crossentropy with \"real\" labels for fake images\n            gen_AB_loss = tf.keras.losses.binary_crossentropy(\n                tf.ones_like(disc_fake_B), disc_fake_B, from_logits=True)\n            gen_BA_loss = tf.keras.losses.binary_crossentropy(\n                tf.ones_like(disc_fake_A), disc_fake_A, from_logits=True)\n            \n            # ========== CYCLE CONSISTENCY LOSS ==========\n            # L1 loss between original and reconstructed images\n            # This prevents mode collapse and ensures meaningful translations\n            cycle_loss = (tf.reduce_mean(tf.abs(real_A - cycled_A)) + \n                         tf.reduce_mean(tf.abs(real_B - cycled_B)))\n            \n            # ========== TOTAL GENERATOR LOSS ==========\n            # Combine adversarial and cycle consistency losses\n            total_gen_loss = gen_AB_loss + gen_BA_loss + LAMBDA * cycle_loss\n            \n            # ========== DISCRIMINATOR LOSSES ==========\n            # Each discriminator learns to distinguish real from fake\n            # Loss = 0.5 * (loss_real + loss_fake)\n            \n            # Discriminator A loss\n            disc_A_loss = (\n                tf.keras.losses.binary_crossentropy(tf.ones_like(disc_real_A), disc_real_A, from_logits=True) +\n                tf.keras.losses.binary_crossentropy(tf.zeros_like(disc_fake_A), disc_fake_A, from_logits=True)\n            ) * 0.5\n            \n            # Discriminator B loss\n            disc_B_loss = (\n                tf.keras.losses.binary_crossentropy(tf.ones_like(disc_real_B), disc_real_B, from_logits=True) +\n                tf.keras.losses.binary_crossentropy(tf.zeros_like(disc_fake_B), disc_fake_B, from_logits=True)\n            ) * 0.5\n        \n        # ========== GRADIENT COMPUTATION AND APPLICATION ==========\n        \n        # Get all generator variables (both generators trained together)\n        gen_vars = self.gen_AB.trainable_variables + self.gen_BA.trainable_variables\n        \n        # Compute gradients for all networks\n        gen_grads = tape.gradient(total_gen_loss, gen_vars)\n        disc_A_grads = tape.gradient(disc_A_loss, self.disc_A.trainable_variables)\n        disc_B_grads = tape.gradient(disc_B_loss, self.disc_B.trainable_variables)\n        \n        # Apply gradients to update network weights\n        self.gen_opt.apply_gradients(zip(gen_grads, gen_vars))\n        self.disc_A_opt.apply_gradients(zip(disc_A_grads, self.disc_A.trainable_variables))\n        self.disc_B_opt.apply_gradients(zip(disc_B_grads, self.disc_B.trainable_variables))\n        \n        # Return loss values for monitoring\n        return {\n            'gen_loss': total_gen_loss,\n            'disc_A_loss': disc_A_loss,\n            'disc_B_loss': disc_B_loss,\n            'cycle_loss': cycle_loss\n        }\n\n# ============================================================================\n# VISUALIZATION AND OUTPUT FUNCTIONS\n# ============================================================================\n\ndef display_sample_predictions(model, photo_ds, num_samples=5):\n    \"\"\"\n    Display sample predictions to visualize model performance.\n    \n    Args:\n        model: Trained CycleGAN model\n        photo_ds: Dataset of photo images\n        num_samples: Number of sample predictions to show\n    \"\"\"\n    print(\"Displaying sample predictions...\")\n    \n    # Create subplot grid: num_samples rows, 2 columns\n    _, ax = plt.subplots(num_samples, 2, figsize=(12, 12))\n    if num_samples == 1:\n        ax = ax.reshape(1, -1)  # Ensure 2D array for consistent indexing\n    \n    # Generate and display predictions\n    for i, img in enumerate(photo_ds.take(num_samples)):\n        # ========== GENERATE PREDICTION ==========\n        # Use generator to create Monet-style version\n        prediction = model.gen_AB(img, training=False)[0].numpy()\n        \n        # Convert from [-1, 1] to [0, 255] range for display\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        \n        # ========== PREPARE ORIGINAL IMAGE ==========\n        # Convert original image for display\n        original = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n        \n        # ========== DISPLAY IMAGES ==========\n        ax[i, 0].imshow(original)\n        ax[i, 1].imshow(prediction)\n        ax[i, 0].set_title(\"Input Photo\")\n        ax[i, 1].set_title(\"Monet-esque\")\n        ax[i, 0].axis(\"off\")  # Remove axis ticks and labels\n        ax[i, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\ndef save_all_generated_images(model, photo_ds):\n    \"\"\"\n    Generate Monet-style versions of all photos and save to disk.\n    \n    Args:\n        model: Trained CycleGAN model\n        photo_ds: Dataset containing all photo images\n        \n    Returns:\n        int: Total number of images generated and saved\n    \"\"\"\n    # Create output directory\n    os.makedirs(\"../images\", exist_ok=True)\n    \n    print(\"Generating and saving Monet-style images...\")\n    i = 1\n    \n    # Process each image in the dataset\n    for img in photo_ds:\n        # ========== GENERATE MONET-STYLE IMAGE ==========\n        # Use the trained generator to transform photo to Monet style\n        prediction = model.gen_AB(img, training=False)[0].numpy()\n        \n        # Convert from [-1, 1] normalized range to [0, 255] pixel values\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        \n        # ========== SAVE IMAGE ==========\n        # Convert numpy array to PIL Image and save as JPEG\n        im = Image.fromarray(prediction)\n        im.save(\"../images/\" + str(i) + \".jpg\")\n        \n        # Progress indicator (print every 200 images)\n        if i % 200 == 0:\n            print(f\"Saved {i} images...\")\n        i += 1\n    \n    total_images = i - 1\n    print(f\"Finished! Saved {total_images} generated images to ../images/\")\n    return total_images\n\ndef create_zip_archive():\n    \"\"\"\n    Create a zip archive of all generated images for easy download.\n    \n    Returns:\n        str or None: Path to created zip file, or None if creation failed\n    \"\"\"\n    try:\n        # Create zip archive of the images directory\n        shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")\n        print(\"Created zip file: /kaggle/working/images.zip\")\n        return \"/kaggle/working/images.zip\"\n    except Exception as e:\n        print(f\"Error creating zip file: {e}\")\n        print(\"You can manually zip the images folder if needed.\")\n        return None\n\n# ============================================================================\n# MAIN TRAINING PIPELINE\n# ============================================================================\n\ndef main():\n    \"\"\"\n    Main function that orchestrates the entire CycleGAN training and inference pipeline.\n    \"\"\"\n    \n    # ========== SETUP ==========\n    print(\"Setting up GPU configuration...\")\n    setup_gpu()\n    \n    # ========== DATA LOADING ==========\n    print(\"Loading datasets from Google Cloud Storage...\")\n    \n    # Get the GCS path provided by Kaggle\n    GCS_PATH = KaggleDatasets().get_gcs_path()\n    print(f\"GCS Path: {GCS_PATH}\")\n    \n    # Load datasets using GCS paths\n    try:\n        # Create file patterns for JPG files in GCS buckets\n        monet_pattern = str(GCS_PATH + '/monet_jpg/*.jpg')    # Monet paintings\n        photo_pattern = str(GCS_PATH + '/photo_jpg/*.jpg')    # Regular photos\n        \n        # Create TensorFlow datasets from the file patterns\n        monet_ds = create_dataset_from_gcs(monet_pattern)\n        photo_ds = create_dataset_from_gcs(photo_pattern)\n        print(\"Datasets loaded successfully from GCS\")\n    except ValueError as e:\n        print(f\"Error loading datasets: {e}\")\n        return\n    \n    # ========== MODEL CREATION ==========\n    print(\"Creating CycleGAN model...\")\n    \n    # Initialize the CycleGAN with all four networks\n    model = SimpleCycleGAN()\n    \n    # Configure optimizers for training\n    # Adam with lr=2e-4 and beta_1=0.5 are standard GAN settings\n    model.compile(\n        gen_opt=tf.keras.optimizers.Adam(2e-4, beta_1=0.5),\n        disc_opt=None  # Not used anymore, we create separate optimizers in compile()\n    )\n    \n    # ========== TRAINING ==========\n    print(\"Starting training...\")\n    \n    # Combine photo and Monet datasets for paired training\n    # Each batch will contain one photo and one Monet painting\n    combined_ds = tf.data.Dataset.zip((photo_ds, monet_ds))\n    \n    # Train the model for specified number of epochs\n    model.fit(combined_ds, epochs=EPOCHS)\n    \n    # ========== INFERENCE AND VISUALIZATION ==========\n    print(\"Training completed! Generating sample predictions...\")\n    \n    # Display sample predictions to visualize results\n    display_sample_predictions(model, photo_ds, num_samples=5)\n    \n    # ========== GENERATE ALL IMAGES ==========\n    print(\"Generating Monet-style versions of all photos...\")\n    \n    # Process entire photo dataset and save generated images\n    total_images = save_all_generated_images(model, photo_ds)\n    \n    # ========== CREATE DOWNLOADABLE ARCHIVE ==========\n    print(\"Creating downloadable archive...\")\n    \n    # Create zip file for easy download\n    zip_file = create_zip_archive()\n    \n    # ========== SUMMARY ==========\n    print(f\"\\n\" + \"=\"*50)\n    print(\"TRAINING AND GENERATION COMPLETED!\")\n    print(\"=\"*50)\n    print(f\"Generated images saved in: ../images/\")\n    print(f\"Total images generated: {total_images}\")\n    if zip_file:\n        print(f\"Zip archive created: {zip_file}\")\n    print(\"=\"*50)\n    \n    print(\"\\nYour CycleGAN has successfully learned to transform photos into Monet-style paintings!\")\n\n# ============================================================================\n# SCRIPT EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    # Run the complete pipeline when script is executed directly\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T03:29:42.459078Z","iopub.execute_input":"2025-06-17T03:29:42.459402Z","iopub.status.idle":"2025-06-17T04:06:14.801344Z","shell.execute_reply.started":"2025-06-17T03:29:42.459376Z","shell.execute_reply":"2025-06-17T04:06:14.800254Z"}},"outputs":[],"execution_count":null}]}